# Introduction

Linear algebra is the study of vectors and linear transformations. It is a fundamental part of mathematics and is used in many fields, including physics, computer science, and engineering. Linear algebra is used to solve systems of linear equations, analyze data, and study geometric objects such as lines, planes, and higher-dimensional spaces.

Here are some resources that I found useful (list in progress):
1. [Practice problems](https://web.pdx.edu/~erdman/LINALG/Linalg_pdf.pdf)

References:
1. [Linear Algebra, An Introduction (2nd Edition)](https://mathematicalolympiads.wordpress.com/wp-content/uploads/2012/08/lineer-algebra2.pdf)
2. [Essence of Linear Algebra, 3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)

I truly believe that knowledge is for everyone — if it weren't for random people on the internet, I wouldn't have been able to learn economics, calculus, computer science, or physics. Now, I'm giving back. I hope you find this useful.

*p.s. This is a work in progress, and there may be errors. If you find any, please let me know!*

<hr >

# Vectors and Coordinate Spaces

**Vector**<br />
The first, most basic concept in linear algebra. A vector can be anything, as we'll soon learn. At its most basic, a vector is a list of scalars. For example, in $\mathbb{R}^2$, a vector is a list of two scalars. In $\mathbb{R}^3$, a vector is a list of three scalars. In $\mathbb{R}^n$, a vector is a list of $n$ scalars. A vector can be represented as a column matrix, or as a list of scalars separated by commas. For example, the vector $\vec{v}=\begin{bmatrix}1\\2\end{bmatrix}$ can also be written as $\vec{v}=\begin{bmatrix}1, 2\end{bmatrix}$.
$$
\vec{v}=\langle5, 0\rangle=
\left[ {\begin{array}{c}
   3\\
   0\\
  \end{array} } \right]
$$

**Real Coordinate Spaces ($\mathbb{R}^n$)**<br />
If you've taken analysis, you're familiar with a **Cartesian coordinate system** — every point is a unique combination of $n$ real numbers. This is a **real coordinate space**. The set of all $n$-tuples of real numbers is denoted $\mathbb{R}^n$. For example, $\mathbb{R}^2$ is the set of all 2-tuples of real numbers, and $\mathbb{R}^3$ is the set of all 3-tuples of real numbers. Here's the general form of a vector in $\mathbb{R}^n$:
$$
\vec{v}\left[ {\begin{array}{c}
a_1 \\
a_2 \\
...\\
a_n
\end{array}}\right]\in\mathbb{R}^n.
$$
Note that the "points" we've always worked with in high school, like $(3, 4)$, are actually vectors in $\mathbb{R}^2$. On another note, there also exist **complex coordinate spaces** ($\mathbb{C}^n$), where the scalars are complex numbers — but we won't focus on them.

**Zero Vector**<br />
The zero vector is a vector whose components are all zero. It is denoted by $\textbf{0}$, and exists in every real coordinate space.

**Adding Vectors**<br />
For vectors with identical dimensions, just add the corresponding components:
$$
\begin{bmatrix} 6\\ -2 \end{bmatrix} + \begin{bmatrix} -4\\ 5\end{bmatrix} = \begin{bmatrix} 2\\ 3\end{bmatrix}
$$
If the dimensions are different, you can (if needed) extend the vector in the lower space by adding zeroes to its higher dimensions (like with polynomials — a quadratic will have a coefficient of $0$ on the $x^5$ term).

**Multiplying by a Scalar**<br />
Multiplying vectors by a scalar is easy! Just multiply each component by the scalar.
$$
2\cdot \left[ \begin{array}{c}
1 \\
2 \\
3
\end{array}\right]=
\left[\begin{array}{c}
2 \\
4 \\
6
\end{array}\right]
$$
**Unit Vector**<br />
A vector with a magnitude of $1$. To normalize vectors, divide each component by the magnitude ($\times \frac{1}{||\vec{v}||}$).

**Standard Basis**<br />
Until we formally cover bases, keep in the back of your mind that every vector in $\mathbb{R}^n$ is a combination of the standard basis vectors (which we'll call $\hat{\textbf{i}}$, $\hat{\textbf{j}}$, $\hat{\textbf{k}}$, etc.). For example, in $\mathbb{R}^2$, the standard basis vectors are, in order, $\begin{bmatrix}1\\0\end{bmatrix}$ and $\begin{bmatrix}0\\1\end{bmatrix}$.

**Dot Product** — $\vec{v}\cdot\vec{w}$
The projection of $\vec{w}$ onto $\vec{v}$ times the length of $\vec{v}$. The dot product can be negative (the length of the projection would be $\times(-1)$ if it were in the opposite direction of $\vec{v}$)! Calculated by multiplying the entries in each vector and adding them up:
$$
\begin{bmatrix}a \\ b \\ c\end{bmatrix}\cdot \begin{bmatrix}d \\ e \\ f\end{bmatrix}=ad+be+cf
$$
Perpendicular vectors have dot product $0$ because the length of the projection is $0$. We can use dot products to determine whether vectors are facing in generally the same direction (dot product $>0$), perpendicular (dot product $=0$), or different directions (dot product $< 0$).
This is just something you may come across in other branches of mathematics, but we won't need dot products much in linear algebra.

<hr />
Now we need to define some terms that will be useful in the future.


**Linear Combination**<br />
If you have some vectors $\vec{v_1}, \vec{v_2},...,\vec{v_n}$ in $\mathbb{R}^m$ (real space). A linear combination is some linear combination of these vectors, where each vector is scaled by a real constant:
$$a_1\vec{v_1}+a_2\vec{v_2}+...+a_n\vec{v_n}.$$
As mentioned earlier, every vector in $\mathbb{R}^n$ can be represented as a linear combination of the standard basis vectors — but we'll see this term pop up a lot more!

**Linear Dependence**<br />
A set that is **linearly dependent** is a set where a member vector can be represented as a linear combination of other vectors in the set (i.e., that vector doesn't add any new "dimensionality" to the set). A more formal definition: a set $S$ is linearly _dependent_ if and only if
$$
c_1\vec{v_1}+c_2\vec{v_2}+...+c_n\vec{v_n}=\textbf{0}
$$
Where not all $c_1...c_n$ are zero. Or, a set is linearly *independent* if the only solution to the above equation is $c_1=c_2=...=c_n=0$.

**Span**<br />
A **span** is defined as the set of all linear combinations of a set $S$ of vectors. For example, two linearly independent vectors span a plane ($\mathbb{R}^2$). The span of a set of collinear $2$-tuples is the line on which they are collinear. Notation:
$$
\text{span}(\left\{ \vec{\textbf{0}}\right\})= \left\{ (0,0) \right\}
$$

<hr >

# Matrices

**Matrices**<br />
Matrices are pretty simple. They're just arrays of column vectors. For example, the matrix
$$
\begin{bmatrix}
1 & 2 & 3\\
4 & 5 & 6
\end{bmatrix}
$$
is a $2\times 3$ matrix. The first row is $\begin{bmatrix}1 & 2 & 3\end{bmatrix}$, and the second row is $\begin{bmatrix}4 & 5 & 6\end{bmatrix}$. The first column is $\begin{bmatrix}1 \\ 4\end{bmatrix}$, the second column is $\begin{bmatrix}2 \\ 5\end{bmatrix}$, and the third column is $\begin{bmatrix}3 \\ 6\end{bmatrix}$.

**Matrix Equivalency**<br />
Two matrices $A$ and $B$ are equivalent if there exist two real numbers $n$ and $p$ such that the following conditions hold:
1. $\text{(Number of rows of }A\text{)}=n=\text{(Number of rows of }B\text{)}$
2. $\text{(Number of columns of }A\text{)}=p=\text{(Number of columns of }B\text{)}$
3. Corresponding indices of $A$ and $B$ are equivalent.

**Matrix Addition**<br />
Matrix addition is defined elementwise — add corresponding elements. Matrix addition is commutative ($A+B=B+A$) and associative ($(A+B)+C=A+(B+C)$). Undefined if the orders of $A$ and $B$ are not the same.

**Scalar Multiplication**<br />
For a matrix $A$ and real number $\lambda$, $\lambda A$ is defined by $(\lambda A)_{i, j}=\lambda A_{i,j}$.
1. $\lambda(A+B)=\lambda A+\lambda B$.
2. $(\lambda_1 + \lambda_2)A=\lambda_1A + \lambda_2 A$.
3. $(\lambda_1\lambda_2)A=\lambda_1(\lambda_2A)$.

**Matrix Multiplication**<br />
Let $p, r$ and $n$ be positive integers, $A$ be a matrix with order $p\times r$, and $B$ be a matrix with order $r\times n$. The **product matrix**, $AB$, is the $p\times n$ matrix $C[c_{i,j}]$ defined by
$$
c_{ij}=\sum_{k=1}^ra_{ik}b_{kj},
$$
For $i=1,2,\dots,p$ and $j=1,2,\dots n$. This means that $c_{i,I}$ is obtained by multiplying the $i$th row of $A$ with the $j$th row of $B$ and summing. The multiplicative identity matrix is the diagonal matrix with only $1$ as its diagonal elements (and is denoted by $I_n$).

If $B$ and $C$ are matrices such that $BC$ is defined, then $\lambda(BC)=B(\lambda C)$. Here's a gif:
<img className="invert" src="https://boydjohnson.dev/blog/concurrency-matrix-multiplication/matrix-multiplication-good.gif"></img>

## Special Types of Matrices

There are some special types of matrices that we'll need to know about.

**Identity Matrix**<br />
The identity matrix is a square matrix with $1$'s on the main diagonal and $0$'s elsewhere. It is denoted by $I_n$ (where $n$ is the order of the matrix). The identity matrix is the multiplicative identity for matrices: $AI_n=I_nA=A$.

**Zero Matrix**<br />
The unique zero matrix of order $p\times n$ is a matrix (of order $p\times n$) whose entries are all zero. It is denoted with $\textbf{0}_{p\times n}$. If $A$ and $\textbf{0}_{p\times n}$ have the same order, then $A+\textbf{0}_{p\times n}=A$.


**Triangular Matrices**<br />
1. **Upper Triangular**: A square matrix is upper triangular if $a_{i,j}=0$ whenever $i > j$.
2. **Lower Triangular**: A square matrix is lower triangular if $a_{i,j}=0$ whenever $i < j$.<br />
An upper triangular matrix has elements that are all $0$ below the main diagonal, and a lower triangular matrix has zero elements above the main diagonal. The product of a UT or LT matrix with another UT or LT (must be the same) matrix is also UT or LT (must be the same).
Here's an example of an upper triangular matrix:
$$
\begin{bmatrix}
1 & 2 & 3\\
0 & 4 & 5\\
0 & 0 & 6
\end{bmatrix}.
$$

**Diagonal Matrix**<br />
A diagonal matrix is a matrix where all the elements are zero except for the diagonal elements. The product of two diagonal matrices is also a diagonal matrix. So, a diagonal matrix is both UT and LT.

**Sub-matrix**<br />
A sub-matrix of a matrix $A$ is any matrix obtained by removing a number of rows or columns from $A$. We'll need this when we talk about the determinant.

**Partitioned Matrix**<br />
A matrix is partitioned if it is divided into submatrices by horizontal and vertical lines between rows and columns. The resulting submatrices are often called **blocks**. Matrices are **compatibly partitioned** if the number of horizontal blocks in $A=$ the number of vertical blocks in $B$ (i.e., the block dimensions are multiplicable) AND each block $\times$ block operation "makes sense" — i.e., the submatrices can be multiplied.
Note that partitioning a matrix doesn't change anything about the matrix — it's just a way to organize the matrix.

**Symmetric Partition**<br />
A symmetrically partitioned matrix is one where the horizontal and vertical partition lines occur in the same places relative to the sequence of rows and columns.

**Diagonal Block**<br />
A diagonal block (in a symmetrically partitioned matrix) can be defined in two ways:
1. A block bounded by the $i$-th and $i+1$st horizontal AND vertical partition lines.
2. A block whose diagonal elements are all diagonal elements of $A$.<br />
Each diagonal block must be square. A **block diagonal matrix** is one whose nondiagonal blocks are all zero submatrices.

## Special Matrix Definitions

**Row Equivalence**<br />
Two matrices are row equivalent if one can be changed to the other by a sequence of elementary row operations.


**Positive Integral Power**<br />
$A^n$ (for any positive integer $n$) is called a "positive integral power of $A$."

**Transpose**<br />
The transpose of a $p\times n$ matrix $A$ is the $n\times p$ matrix $A^T$ such that
$$
(A^T)_{i,j}=A_{j,i}.
$$
Essentially, each row of $A$ becomes a column of $A^T$. Transposes have the following properties:
$$
\begin{align}
(A^T)^T&=A\\
(\lambda A)^T&=\lambda(A^T)\\
(A+B)^T&=A^T+B^T\\
(AB)^T&=B^TA^T\\
{\left(A^n\right)}^T&={\left(A^T\right)}^n
\end{align}
$$

**Symmetry**<br />
A matrix is *symmetric* if $A^T=A$. This can be possible if and only if $A$ is square and $A_{ij}=A_{ji}$. So, the main diagonal can be anything, but the other elements must be symmetric.

**Skew Symmetry**<br />
A matrix $A$ is *skew symmetric* if $A^T=-A$. This is true if and only if $A$ is square and $A_{ij}=-A_{ji}$. A skew symmetric matrix must have all diagonal elements equal to $0$.
It is a good exercise to prove that a skew symmetric matrix $A$ squared is symmetric, and $A^3$ is skew symmetric.


<hr >

# Linear Systems of Equations

**Linear Systems of Equations**<br />
A system of equations (you know this), where every term is a scalar multiple of a variable. Then, each equation models a linear combination of the variables.

<img className="invert" src="https://cloud-fpput427k-hack-club-bot.vercel.app/0image.png"></img>
_Credit to 3Blue1Brown._

This can be written as $A\cdot \vec{x}=\vec{v}$. So the solution to a system of linear equations is just a vector that, when transformed by $A$, gives $\vec{v}$! How do we solve a system of linear equations (SLE)? We can use Gaussian elimination: we write the SLE as an augmented matrix, row reduce it, and then solve the resulting system of equations.<br />
First, let's note that a system of linear equations will have either zero, one, or infinitely many solutions:

Theorem: Suppose $x_1$ and $x_2$ are solutions to the matrix equation $A\vec{x}=\vec{b}$. If $\alpha$ and $\beta$ are any real numbers such that $\alpha + \beta=1$, then $\alpha x_1 + \beta x_2$ is also a solution to the equation.

Here are some important terms we need before we can solve SLEs:

**Consistent**<br />
A linear system of equations is **consistent** if it has at least one solution. Otherwise, it is inconsistent.

**Homogenous**<br />
A system of linear equations corresponding the matrix equation $A\vec{x}=\vec{b}$ is **homogenous** if $\vec{b}=\vec{\textbf{0}}$. Otherwise, it is inhomegenous. Every homogenous system is consistent ($\vec{x}=\vec{\textbf{0}}$, which is called the **trivial solution**).

**Row-Reduced Form**<br />
A matrix is in row-reduced form if it satisfies the following four conditions:
1. All zero rows (if any) appear below all nonzero rows.
2. The first nonzero element in any nonzero row is $1$.
3. All elements below and in the same column as the first nonzero element in any nonzero row are $0$.
4. The first nonzero element of any nonzero row appears in a column further (farther??) to the right than the first nonzero element in any preceding row.

Ex. $A=\begin{bmatrix}1 & 7 & -1 \\ 0 & 1 & 6 \\ 0 & 0 & 0\end{bmatrix}$ is in row-reduced form.

**Pivot**<br />
If a matrix is in row-echelon form, then the first nonzero element of each row is called a **pivot**, and a column in which a pivot appears is called a **pivot column**. In a system of linear equations, the pivot columns represent **basic variables**. Non-basic variables (ones that are not in a pivot column) are called **free variables**. Free variables can take on *any* value (we are "free" to choose any value)

Now we're ready to solve SLEs!

The first thing we need to do is form an **augmented matrix**. This is a matrix that has the coefficients of the variables on the left and the constants on the right. For example, the augmented matrix of the system
$$
\begin{align}
x_1+2x_2+3x_3&=4\\
2x_1+5x_2+2x_3&=3\\
3x_1+7x_2+8x_3&=1
\end{align}
$$
is
$$
\begin{bmatrix}
1 & 2 & 3 & 4\\
2 & 5 & 2 & 3\\
3 & 7 & 8 & 1
\end{bmatrix}.
$$

<hr />

Once we've constructed this augmented matrix, we can use **Gaussian elimination** to row reduce it. This involves using elementary row operations to transform the augmented matrix into row-reduced form.

**Guassian Elimination**<br />
First, note that the following three operations do not change the set of solutions:
1. $R_1$ — Changing the order of equations.
2. $R_2$ — Multiplying an equation by a nonzero scalar.
3. $R_3$ — Adding to one a equation a scalar times another equation.

These are called **elementary row operations**. We'll talk more about them in a later section.
The **augmented matrix of the system** $A\vec{x}=\vec{b}$ is $\left [A\;\textbf{|}\;\vec{b} \right]$. Guassian elimination has 4 steps:
1. Write the system of equations as an augmented matrix.
2. Use elementary row operations to transform the augmented matrix into row-reduced form.
3. Write the equations corresponding to the reduced augmented matrix.
4. Solve the new set of equations with back-substitution.

Here are some general guidelines for working with elementary row operations:
- Completely transform one column into the required form before starting on another.
- Work the columns in order from left to right.
- Never use an operation that changes a $0$ in a previous column.

If the number $n$ of variables is greater than the number $r$ of variables, then the $r$ equations determine the values of $r$ variables, and the remaining $n-r$ variables have no restrictions. Thus, there are infinitely many solutions.
Once we've row-reduced the augmented matrix, we can write the system of equations corresponding to the reduced augmented matrix. We can then solve this system of equations using back-substitution. For example:
$$
\begin{align}
x_1+2x_2+3x_3&=4\\
2x_1+2x_2+2x_3&=2\\
3x_1+6x_2+9x_3&=12
\end{align}
$$
The augmented matrix is
$$
\begin{bmatrix}
1 & 2 & 3 & 4\\
2 & 2 & 2 & 2\\
3 & 6 & 9 & 12
\end{bmatrix}.
$$
The row-reduced form of the augmented matrix is
$$
\begin{bmatrix}
1 & 2 & 3 & 4\\
0 & 1 & 2 & 3\\
0 & 0 & 0 & 0
\end{bmatrix}.
$$
The corresponding system of equations is
$$
\begin{align}
x_1+2x_2+3x_3&=4\\
x_2+2x_3&=3\\
0&=0
\end{align}
$$
The last equation is a **trivial equation** and doesn't provide any new information. So, we can ignore it. We can then solve the remaining two equations using back-substitution.

**Elementary Matrices**<br />
An elementary matrix $E$ is a square matrix such that multiplying it by $A$ on the left is the same as applying an elementary row operation to $A$. Elementary matrices are easily constructed by applying the corresponding elementary row operation to the identity matrix of the same order.
So, the elementary matrix that corresponds to multiplying the first row of an $n\times n$ matrix by $k$ is
$$
\begin{bmatrix}
k & 0 & 0 & \dots & 0\\
0 & 1 & 0 & \dots & 0\\
0 & 0 & 1 & \dots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \dots & 1
\end{bmatrix}.
$$


**LU Decomposition**<br />
If a nonsingular (invertible) matrix $A$ can be written as the product of a lower triangular matrix on the left and an upper triangular matrix on the right, $A$ is said to have an LU decomposition. Let $R_3(i,j,k)$ denote the elementary row operation that adds $k$ times the $j$th row to $i$ where $k\neq 0$ and $i\neq j$. We know that the matrix $R_3(i,j,k)$ has all diagonal elements equal to $1$, element at $(i,j)$ equal to $k$, and $0$ everywhere else. So, if $i>j$, $R_3(i,j,k)$ is a lower triangular matrix. All diagonal elements of a square $n\times n$ nonsingular lower triangular matrix $L$ must be nonzero. A nonsingular $n\times n$ matrix $A$ has an LU decomposition if and only if $A$ can be transformed into an upper triangular matrix with $R_3(i,j,k)$ where $i>j$. A square matrix $A$ has an LU decomposition if $A$ can be transformed to an upper triangular matrix using only the third elementary row operation. If $A$ has an LU decomposition, we can solve the equation $A\vec{x}=\vec{b}$ by solving $L\vec{y}=\vec{b}$ and the equation $U\vec{x}=\vec{y}$. Then, we would have $LU\vec{x}=\vec{b}$, $L\vec{y}=\vec{b}$.


<hr >

# Vector Spaces

**What are vectors?**<br />
They're arrows in space, right? This is what most people are taught in high school-level mathematics.

But. There are such things as...

**Vector-ish Thingies**<br />
If you think about it, *functions* are just like vectors, no? You can add them. You can scale them. In essence, functions are just vectors with infinitely many coordinates, where the base is $x^\infty, \dots, x^2, x, 1$ instead of $\begin{bmatrix}1 & 0 & \dots\end{bmatrix}^T,\begin{bmatrix}0 & 1 & \dots \end{bmatrix}^T$.
With this basis (for polynomials of degree $n$ or lower, the basis is called the **standard basis of $\mathbb{P}^n$**), you can represent any polynomial as a linear combination of the basis vectors. For example, the polynomial $3x^2+2x+1$ can be represented as $\begin{bmatrix}3 & 2 & 1\end{bmatrix}^T$.
Back to the question — what are vectors? The answer is, anything that follows the rules. Linear algebra is defined by 10 axioms, and all of the results that can be called "linear algebra" follow as a result of these 10 axioms. So, it *doesn't matter* what a "vector space" is — so long as it follows the axioms, the findings hold true.

**Formal Definition of a Vector Space**<br />
A vector space is a set $V$ of objects (vectors) and a set of scalars (either $\mathbb{R}$ or $\mathbb{C}$), together with two binary operations, vector addition $\oplus$ and scalar multiplication $\odot$, that satisfy the following properties:
- Addition:
    - (A1) Closure: For all $u,v\in \mathbb{V},$ $u\oplus v\in \mathbb{V}$.
    - (A2) Commutativity: For all $u,v\in \mathbb{V}$, $u\oplus v=v\oplus u$.
    - (A3) Associativity: For all $u,v,w\in \mathbb{V}$, $(u\oplus v)\oplus w=u\oplus (v\oplus w)$.
    - (A4) Existence of an identity: There exists a zero vector in $\mathbb{V}$ (denoted $\textbf{0}$) such that for every $v\in \mathbb{V}$, $v\oplus \textbf{0}=v$.
    - (A5) Existence of inverse: There exists for every $v\in \mathbb{V}$ a $-v$ such that $v\oplus (-v)=0$.
- Multiplication:
    - (S1) Closure: For all $u\in \mathbb{V}$ and all scalars $\alpha$, $\alpha\odot u\in \mathbb{V}$.
    - (S2) Associativity: For all $u\in \mathbb{V}$ and scalars $\alpha, \beta$, $\alpha \odot(\beta\odot u)=(\alpha\cdot\beta)\odot u$.
    - (S3) Non-scaling property: $1$ is an identity element. For all vectors $u\in \mathbb{V}$, $1\odot u=u$.
    - (S4) Distributivity (1): For all $u\in \mathbb{V}$ and scalars $\alpha$ and $\beta$, $(\alpha + \beta)\odot u=\alpha\odot u \oplus \beta\odot u$. Note that "$+$" refers to standard addition of real and complex numbers.
    - (S5) Distributivity (2): For all $u, v\in \mathbb{V}$ and scalars $\alpha$, $\alpha\odot(u\oplus v)=\alpha\odot u \oplus \alpha\odot v$.

If $\mathbb{R}$ is the set of scalars for a vector space, that vector space is said to be a **real vector space**. If the set of scalars is $\mathbb{C}$, the vector space is said to be a **complex vector space**, or a **vector space over $\mathbb{C}$**. Any vector space defined over $\mathbb{C}$ with dimension $n$ can be viewed as a set over $\mathbb{R}$ with dimension $2n$.


**Subspaces**<br />
A **subspace of $\mathbb{R}^n$** is some subset $V$ of $\mathbb{R}^n$ that satisfies the following properties:
1. Identity: $\vec{\textbf{0}}\in \mathbb{V}$.
2. Closure under multiplication: $\left\{c\cdot\vec{x}\in \mathbb{V}\; \forall\; c\in \mathbb{R},\,\vec{x}\in \mathbb{V}\right\}$.
3. Closer under addition: $\left\{\vec{a}+\vec{b}\in \mathbb{V} \;\forall\; \vec{a}, \vec{b}\in \mathbb{V} \right\}$.
4. Non-emptiness: $\mathbb{V}\neq \emptyset$.

These 4 properties can be simplified to one, closure under linear combination:
$$
\text{For all } \vec{v}, \vec{w}\in \mathbb{V} \text{ and scalars } c, d, c\vec{v}+d\vec{w}\in \mathbb{V}.
$$
As it implies the 4 hold true. We'll see subspaces used more when we talk about eigenvectors and eigenvalues.

**Properties of Vector Spaces**<br />
- In any vector space, there is a unique zero vector.
- Additive inverses are unique.
- For any $v\in \mathbb{V}$, if $v\oplus v=v$, $v=\textbf{0}$.
- For any $u\in \mathbb{V}$, $u\odot \textbf{0}=\textbf{0}$.
- For any scalar $\alpha$, $\alpha\odot \textbf{0}=\textbf{0}$.
- For any $v\in \mathbb{V}$, $(-1)\odot v=-v$.
- For any $v\in \mathbb{V}$, $v=-(-v)$.
- For any $v\in \mathbb{V}$ and scalar $\alpha$, if $a\odot v=0$, either $a=0$ or $v=\textbf{0}$.



**Spanning Set**<br />
A set $S$ is a spanning set of $\mathbb{V}$ if $span(S)=\mathbb{V}$. This means that "$S$ spans $\mathbb{V}$." If $S\subset \mathbb{V}$, then:
1. If $span(S)=\mathbb{V}$, then some subset of $S$ is a basis for $\mathbb{V}$.
2. If $S$ is linearly independent, then $S$ is a subset of some basis for $\mathbb{V}$.

Lemmas:
1. If $\mathbb{V}$ is a vector space and $S\subset T\subset \mathbb{V}$, then $span(S)\subset span(T)$.
2. If $\mathbb{V}$ is a vector space and $S\subset \mathbb{V}$, then $span(span(S))=span(S)$.
3. If $\mathbb{V}$ is a vector space and $S,T\subset \mathbb{V}$ such that $S\subset T$, then $span(S)\subset span(T)$.
4. If $v_1,\dots, v_n$ are vectors in vector space $\mathbb{V}$, then
    1. $span\{v_1, \dots, v_k, \dots, v_n\}=span\{v_1,\dots, v_k+\lambda v_j, \dots, v_n\}$
    2. $span\{v_1, \dots, v_k, \dots, v_n\}=span\{v_1,\lambda v_k, \dots, v_n\}$ (where $\lambda\in\mathbb{R}$ and $\lambda \neq 0$).

**Basis**<br />
A **basis** for $\mathbb{V}$ is a *linearly independent* spanning set of $V$. Basically a "minimal" spanning set. A vector space is **finitely dimensional** if it has a basis with finitely many elements.

Thm. If the set $S=\left\{ v_1,v_2,\dots,v_n \right\}$ spans $\mathbb{V}$, then any set $T$ with more than $n$ elements must be linearly dependent. From this it follows that if $B=\left\{v_1, v_2,\dots, v_n \right\}$ is a basis for $\mathbb{V}$, then every linearly dependent subset of $\mathbb{V}$ must have $n$ or fewer elements. Also, every basis for $\mathbb{V}$ has the same number of elements. The number of elements in a basis for $\mathbb{V}$ is called the **dimension** of $\mathbb{V}$.

Bases are used to express any vector in the vector space. For any vector $\vec{u}\in \mathbb{V}$, there is only one way to express $\vec{u}$ as a linear combination of the basis vectors. The coefficients used in this linear combination are called the **coordinates**, and can be written as $n$-tuples or column matrices.
We'll use this a lot later when we talk about transformations and change of basis operations.

<hr />

I wanted to talk a bit more about why we might want to use polynomials as vectors (or anything, for that matter). The truth is, linear algebra is a tool — a *very* powerful one. Consider taking the derivative of a polynomial.
There is no "strictly math" way to implement it, in, say, Python. But if we consider the polynomial as a vector, we can take the derivative of the polynomial by simple matrix multiplication — which is easy-peasy in Python.
Then, the derivative could be defined as
$$
A=\begin{bmatrix}0 & 1 & 0 & 0 & 0&\dots\\0 & 0 & 2 & 0 & 0&\dots\\0 & 0 & 0 & 3 & 0&\dots\\0 & 0 & 0 & 0 & 4 & \dots\\\vdots & \vdots & \vdots & \vdots & \vdots & \ddots\end{bmatrix},
$$
with respect to the basis $\left\{1, x, x^2, x^3, \dots\right\}$. So, the derivative of the polynomial $p(x)=1+2x+3x^2$ is just $Ap(x)=\begin{bmatrix}0&2&6&0&\dots\end{bmatrix}^T=2+6x$.
This is a very powerful tool, and it's why we use linear algebra. It's not just about vectors and matrices — it's about the power of abstraction and the ability to manipulate things in ways that we couldn't before.

If you're not sure how we came up with $A$, you'll see it later when we talk about transformations. For now, just know that it's a way to represent the derivative of a polynomial as a matrix.

<hr >

# Rank, Nullity, and Dimension

**Row Rank**<br />
The row rank of a matrix is the dimension of the row space of the matrix. The row rank of a matrix is the maximum number of linearly independent rows in the matrix.
Unsurprisingly, the row rank is denoted $rowrank(A)$.

**Column Rank**<br />
The column rank of a matrix is the dimension of the column space of the matrix. The column rank of a matrix is the maximum number of linearly independent columns in the matrix.
If you had to guess how the column rank is denoted, what would you say? $colrank(A)$? You're right!

_If you didn't say $colrank(A)$...I don't know what to tell you._

**Rank**<br />
The rank of a matrix is, interestingly, the same as the row rank and the column rank.
There are a number of properties of the rank of a matrix:
- The system $A\vec{x}=\vec{b}$ is consistent if and only if $r(A)=r(\begin{bmatrix}A \,| \,\, b\end{bmatrix})$. This is clear because it means that the columns of $A$ with $b$ are linearly dependent, so $b$ can be expressed as a linear combination of $A$'s columns.
- An $n\times n$ matrix $A$ is invertible if and only if $r(A)=n$ ($A$ can be row-reduced to the identity matrix).
- If we have an consistent $n$-variable SLE $Ax=b$, then the solutions to the system are expressible in $n-r(A)$ unknowns (the number of zero rows after row-reducing).
- If $AB=I$, then the rows of $A$ are linearly independent, and $r(A)=n$.
- If $A$ is an $n\times n$ matrix such that $r(A)=n$, then there exists matrix $C$ such that $CA=I_n$.
- A square matrix is invertible if and only if it can be transformed to an upper triangular matrix with all diagonal elements equal to $1$ (no zero rows).
- $r(AB)\leq \min(r(A), r(B))$.

**Column Space**<br />
Column space is the set of all possible outputs of $A\cdot\vec{v}$. This is just the *span* of the columns (the basis vectors)! For any $p\times n$ matrix $A$, $colrank(A)=rowrank(A)$. This means that the rank is equal to the column rank and the row rank. We can find bases for the column space in one of three ways:
1. By inspection.
2. By transposing, row-reducing, and taking nonzero columns.
3. By row-reducing (to $R$) and taking pivotal columns in $R$'s corresponding columns in $A$.

**Row Space**<br />
The row space of a an $m\times n$ matrix $A$ is the span of the rows of $A$ each viewed as a matrix in $\mathbb{R}^n$. The **row rank** of a matrix is the dimension of the row space. The two are denoted $rowspace(A)$ and $rowrank(A)$. The basis of the row space of a matrix in row-reduced form is the nonzero rows of that matrix, and the row rank is the number of nonzero rows. Even better: if a matrix $B$ is obtained from matrix $A$ by applying some elementary row operations, then $rowspace(A)=rowspace(B)$. Also, if $B$ has some LI columns, the corresponding columns in $A$ are LI.

**Full Rank**<br />
Not a poker hand! **Full rank** is when the rank of a transformation is the same as the number of input dimensions (number of columns).

**Finding a Basis for the...**<br />
1. **Row Space**: Find the row-reduced form of the matrix. The basis is the set of nonzero rows.
2. **Image/Column Space**: Find the row-reduced form of the transpose of the matrix. The basis is the set of nonzero rows.
3. **Kernel/Null Space**: Find the row-reduced form of the matrix. The basis is the set of special solutions to the system $Ax=\textbf{0}$. Here, row-reducing isn't actually necessary, but it's helpful to solve $Ax=\textbf{0}$

<hr >

# Transformations

**Linear Transformation**<br />
A transformation is a function that takes in a vector and outputs another vector — not at all different from our typical AP Calculus BC-style vector-valued functions. Now, however, we're working with vectors that could be *anything*, not just Cartesian coordinates.
To denote that a transformation $T$ has domain $\mathbb{V}$ and range $\mathbb{W}$, we write $T:\mathbb{V}\to \mathbb{W}$. A transformation $T:\mathbb{V}\to \mathbb{W}$ (mathematically) is **linear** if for all scalars $v_1, v_2$ and scalars $\alpha,\beta$:
$$
T(\alpha v_1 + \beta v_2) = \alpha T(v_1)+\beta T(v_2).
$$
For any vector space $\mathbb{V}$ with basis $B$, the transformation $\phi:\mathbb{V}\to \mathbb{R}^n$ defined by $\phi(v)=(v)_B$ is linear, one-to-one, and onto, where $(v)_B$ is the coordinate vector of $v$ with respect to $B$.

How do we describe these numerically? An interesting observation is that we only need to follow where the basis vectors end up — since the linear transformation preserves addition and multiplication (linear combination), a vector like $\vec{v}=2\hat{\textbf{i}}+3\hat{\textbf{j}}$ transformed is the _same linear combination_ of the transformed $\hat{\textbf{i}}$ and $\hat{\textbf{j}}$ as the original vector was of $\hat{\textbf{i}}$ and $\hat{\textbf{j}}$! So, a linear transformation is completely described by  the coordinates where $\hat{\textbf{i}}$ and $\hat{\textbf{j}}$ land. This can be packaged into a matrix where each column describes the transformed locations of  $\hat{\textbf{i}}$ and $\hat{\textbf{j}}$. In the most general ($2\times 2$) case:

$$
\begin{bmatrix}a & b \\ c & d\end{bmatrix}\cdot\begin{bmatrix}x \\ y\end{bmatrix}=\begin{bmatrix}ax+by\\cx + dy\end{bmatrix}
$$

Ex.
$$
\begin{align*}
\text{For the 2x2 matrix given, find where }&\left[\begin{array}{c}5 \\ 7 \end{array}\right] \text{is transformed to.} \newline
\begin{bmatrix}
3 & 2 \\
-2 & 1
\end{bmatrix}\cdot\begin{bmatrix}5 \\ 7\end{bmatrix}&=5\cdot\begin{bmatrix}3 \\ -2\end{bmatrix}+7\cdot\begin{bmatrix}2 \\ 1\end{bmatrix}\\
&=\begin{bmatrix}15\\-10\end{bmatrix}+\begin{bmatrix}14\\7\end{bmatrix}\\
&=\boxed{\begin{bmatrix}29\\-3\end{bmatrix}}
\end{align*}
$$
If $\mathbb{V}$ and $\mathbb{W}$ are vector spaces with bases $B$ and $C$, respectively, and $T:\mathbb{V}\to \mathbb{W}$ is linear, then there exists an $m\times n$ matrix $A$ such that for every vector $v\in \mathbb{V}$, $(Tv)_C=A(v)_B$. This matrix is obviously dependent on the bases $B$ and $C$; to reflect this dependence, we write $A_B^C$, where the subscript reflects the chosen basis for the *domain* of the transformation, and the superscript reflects the chosen basis for the *range* of the basis.

**Image of a Transformation**<br />
The image of a transformation $T:\mathbb{V}\to \mathbb{W}$ is  denoted $image(T)$, and is the subset of $\mathbb{W}$ of all $\left\{ Tv\; |\; v\in \mathbb{V} \right\}$. For any linear transformation $T:\mathbb{V}\to \mathbb{W}$,
1. $\textbf{0}\in image(T)$.
2. $image(T)$ is a subspace of $W$.
	1. $T$ is onto if and only if $r(T)=\dim(\mathbb{W})$.
	
If $\dim(\mathbb{V})=\dim(\mathbb{W})$ and $T:\mathbb{V}\to \mathbb{W}$ is linear, then $T$ is onto if and only if $T$ is one-to-one.

**Rank**<br />
Some squishes (\**sigh*\*... transformations) are more "squishy" than others — 3d space squished into a line is more "squished" than 3d space squished into a plane, but both transformations have determinant $0$. The **rank** describes this — it is the number of dimensions in the output. A transformation whose output is a line has a rank of $1$, while a transformation whose output is a plane has a rank of $2$.
Formally, the rank of a linear transformation $T:\mathbb{V}\to \mathbb{W}$ is
$$
r(T)=\dim(image(T)).
$$

**Null Space/Kernel**<br />
The set of vectors that get transformed into the origin during a transformation. For a transformation with full rank, the null space is just the zero vector. For a rank zero transformation, the null space is $n$-dimensional space. The kernel (or nullspace) of a transformation is:
$$
\ker(T)=\left\{v\in \mathbb{V} \;|\; Tv=0\right\}
$$
For any transformation $T:\mathbb{V}\to \mathbb{W}$,
1. $\textbf{0}\in \ker(T)$.
2. $\ker(T)$ is a subspace of $\mathbb{V}$.
If $V$ is finite dimensional and $T:\mathbb{V}\to \mathbb{W}$ is linear, then $r(T)+null(T)=\dim(\mathbb{V})$. A transformation is one-to-one if and only if $\ker(T)=\left\{\textbf{0}\right\}$.


**Nullity**<br />
The dimension of the nullspace of $T$ is called the **nullity** of $T$, and is denoted by $null(T)$.

**Injective**<br />
A transformation $T$ is **injective** (one-to-one) if for all vectors $v_1, v_2\in \mathbb{V}$, $v_1\neq v_2$ implies $Tv_1\neq Tv_2$.

**Surjective**<br />
A transformation $T$ is **onto** (or surjective) if $\mathbb{W}=image(T)$. So $T$ is onto $\mathbb{W}$ if every element in the range is in the image of $T$.

**Isomorphic**<br />
If $T:\mathbb{V}\to \mathbb{W}$ is linear, onto, and one-to-one, then it is **isomorphic**, and an isomorphism exists between $\mathbb{V}$ and $\mathbb{W}$, denoted $\mathbb{V}\cong \mathbb{W}$. It is reflective, transitive, and symmetric. Since isomorphism is an **equivalence relation**, $\mathbb{V}\cong \mathbb{W}$ if and only if $\dim(\mathbb{V})=\dim(\mathbb{W})$ (there exists an isomorphism $T_1:\mathbb{V}\to \mathbb{R}^n$ and $T_2:\mathbb{W}\to \mathbb{R}^n$). This means that any $n$-dimensional vector space is just a copy of any other $n$-dimensional vector space, and any vector space (up to the names of its elements) is *completely determined* by its dimension.

**Important Linear Transformations**<br />
- $90^\circ$ rotation counterclockwise:
	- $\begin{bmatrix}0 & 1\\-1 & 0\end{bmatrix}$
- Shear ($\hat{\textbf{i}}$ remains fixed and $\hat{\textbf{j}}$ moves to $(1, 1)$):
	- $\begin{bmatrix}1 & 1\\ 0 & 1\end{bmatrix}$
- Counterclockwise rotation by $\theta$ around the origin:
	- $\begin{bmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{bmatrix}$


**Similar Matrices**<br />
Two matrices $A$ and $B$ are **similar** if there exists an invertible matrix $P$ such that $B=P^{-1}AP$. Similar matrices have the same rank, determinant, and trace.
As we'll see when we get to change of basis and determinants, similar matrices are matrices that represent the same linear transformation, but with different bases.
Similarity (denoted $A\cong B$) is transitive, symmetric, and reflexive.

**Transformation Composition (Matrix Multiplication)**<br />
Applying one transformation, then another, is still a linear transformation! So, there is a matrix that describes this **composition** of transformations, and we can call it the "product" of the two original matrices. However, since linear transformations aren't commutative (and are functions), we read matrix multiplication from right to left (just like we read $f(g(x))$). So how do we multiply matrices $A$ and $B$?
$$
A=\begin{bmatrix}a & b \\ c & d\end{bmatrix}, B=\begin{bmatrix}e & f \\ g & h\end{bmatrix}.
$$
$$
M=B\cdot A
$$
There are two ways to think about it. The first way is the intuitive way: the first column of $A$ shows where $\hat{\textbf{i}}$ is transformed to, and the second shows where $\hat{\textbf{j}}$ is transformed to. Then, just multiply the two vectors by $B$ to get where the two transformed vectors go:
$$
\begin{align*}
\begin{bmatrix}e & f\\ g & h\end{bmatrix}\cdot\begin{bmatrix}a\\c\end{bmatrix}=\begin{bmatrix}ae + cf\\ ag + ch\end{bmatrix}\\\\
\begin{bmatrix}e & f\\g & h\end{bmatrix}\cdot\begin{bmatrix}b\\d\end{bmatrix}=\begin{bmatrix}be+df\\bg+dh\end{bmatrix}\\\\
\text{so, the composed matrix is}\\
\boxed{\begin{bmatrix}ae+cf & be+df\\ag+ch&bg+dh\end{bmatrix}}
\end{align*}
$$
And this shows us the second way of thinking about multiplying matrices: memorize the formula!

_P.S. I do not support the second way._

<hr />

If you recall, in an earlier section we talked about the derivative as a linear transformation, denoted by a matrix with respect to the standard basis of $\mathbb{P}^n$. Do you see how we arrived at that matrix now? We just followed where each basis vector went!


<hr >
# Non-square Matrices

...As transformations between dimensions.

**Non-Square Matrices**<br />
A transformation can reasonably transform inputs into higher dimensions (for example, rotate $90^\circ$ around the $x$-axis) — this would be a nonsquare matrix! Here's an example of one that takes a 2d space and transforms it into a 3d space. Notice that the column space (the span of where the vectors land) has the same number of dimensions as the input space — so the transformation is a full rank transformation.
$$
A=\begin{bmatrix}2 & 0 \\ -1 & 1 \\ -2 & 1\end{bmatrix}
$$
This is a $3\times2$ matrix, and the two columns show that there are 2 basis vectors mapped into three dimensions. A $2\times3$ matrix transforms 3d space into a two-dimensional space, and so it is not full rank.

<hr >

# The Determinant

Transformations tend to stretch or shrink (squish) space. So how do we measure *by how much* it shrinks? We can measure the factor by which a given area increases or decreases.

**Notation**<br />
Let $\mu_{ij}$ be the $(n-1)\times(n-1)$ submatrix of $A$ obtained by removing the $i$th row and $j$th column of $A$.

**Determinant**<br />
The **determinant** is the factor by which a given transformation changes any area. The determinant is $0$ if the transform transforms space into a lower dimension (for example, $\mathbb{R}^3$ is transformed into a plane, or even a line). Well... ish. Sorry. The determinant can be negative! This is when space is "inverted".

Ex.
$$
\det\left({\begin{bmatrix}3 & 0\\0 & 2\end{bmatrix}}\right)=6.
$$
Do you see why? We stretch the $y$-axis by $3$, and the $x$-axis by $2$, so the area of any shape is stretched by $6$.

The determinant ($\det(A)=|A|$) is more formally defined recursively:
1. If $A$ is the $1\times 1$ matrix $A=[a_{11}]$, then $\det(A)=a_{11}$.
2. If $A$ is an $n\times n$ matrix (where $n > 1$), then
$$\det(A)=\sum_{j=1}^n (-1)^{j + 1}a_{1j} \det(\mu_{1j}(A)).$$

For a matrix $A=[a_{ij}]$, the scalar quantity $\det(\mu_{ij}(A))$ is called a **minor** of $A$, and $(-1)^{j+i}\det(\mu_{ij}(A))$ is called a **cofactor**. To compute the determinant of $A$, we can
1. Compute the cofactor of each element in the first row.
2. Multiply each element in the first row by its cofactor and sum the results.

We can do this for any row or column, not just the first one. This obviously means that if a matrix has a zero row, the determinant is zero. Similarly, for an upper triangular matrix, the determinant is the product of the diagonal elements. The same is true for diagonal and lower triangular matrices.

**Properties**<br />
- $(u_{ij}(A))^T=u_{ji}(A^T)$. Therefore, for any square matrix $A$, $\det(A)=\det(A^T)$.
- If $A$ has two identical rows or columns, $\det(A)=0$. That is, if $r(A) < n$, $\det(A)=0$.
- If $E$ is the elementary matrix corresponding to interchanging two rows of $A$, then $\det(E)=-1$ and $\det(EA)=-\det(A)=\det(E)\det(A)$.
- If $E$ is the elementary row matrix that corresponds to multiplying a row of $A$ by a scalar $\lambda$, then $\det(E)=\lambda$ and $\det(EA)=\lambda\det(A)$.
- If $E$ is the elementary row matrix corresponding to adding $\lambda$ times row $i$ of $A$ to row $j$, then $\det(E)=1$ and $\det(EA)=\det(A)$.
- For any two $n\times n$ matrices $A$ and $B$, $\det(AB)=\det(A)\det(B)$.
	- From this theorem, it follows that if $A$ is an invertible matrix, $\det(A^{-1})=(\det(A))^{-1}$.
	- Also, similar matrices have the same determinants (proven by taking determinants of $A=P^{-1}BP$).
		- Note that this means if $\mathbb{V}$ is a finite-dimensional vector space and $T:\mathbb{V}\to\mathbb{V}$ is linear, then $\det(T)$, defined as the determinant of any matrix representation of $T$, is a well-defined scalar, independent of the choice of basis.

A square matrix $A$ is nonsingular if and only if its determinant is nonzero. There's a proof, but think of it this way: a zero determinant decreases the dimension of space (and therefore the kernel is nontrivial). That means that the transformation isn't invertible. The proof (vaguely) relies on the facts that no elementary row operation has a zero determinant, and therefore if $A$ is singular, it can be transformed to an upper triangular matrix $B$ with at least one zero on its diagonal (meaning it has a zero determinant), so the determinant of $A$ is $0$.

<hr >
# Inverses


**Inverse of a Matrix**<br />
The "opposite" of a matrix or transformation, $A^{-1}$ (inverse of $A$) has the property that applying $A$, then $A^{-1}$ gives the identity transformation. So, $A^{-1}\cdot A=I_n$. Notes:
- $AB=BA=I_n$. $B$ is the inverse of $A$, and $A$ is the inverse of $B$.
- Non-invertible matrices are called **singular**. Invertible ones are called **non-singular**.
- Inverses are unique.
- The following properties are true for invertible matrices $A$ and $B$:
	- $(A^{-1})^{-1}=A$
	- $(AB)^{-1}=B^{-1}A^{-1}$
	- $(A^T)^{-1}=(A^{-1})^T$
	- $(\lambda A)^{-1}=\frac{1}{\lambda}A^{-1}$

**Computing the Inverse of a Matrix**<br />
Lemma 1. A square matrix is invertible if and only if it can be transformed by elementary row operations to row-reduced form with all diagonal entries nonzero.

Lemma 2. Any row-reduced square matrix with all diagonal elements nonzero can be transformed to the identity matrix with elementary row operations.

By these lemmas, we can write
$$E_{k}E_{k-1}\dots E_2E_1A=I_n,$$
so $A^{-1}=E_{k}E_{k-1}\dots E_2E_1$. So, we can compute the inverse of matrix $A$ by applying the elementary row operations $E_k,\dots, E_1$ to the identity matrix! So, to compute $A^{-1}$, apply the same row operations to the identity matrix as you applied to $A$ in order to transform it to the identity.
One way of finding the inverse of an $n\times n$ matrix $A$ is as follows:
1. Form the $n\times 2n$ matrix $[A\,|\,I_n]$.
2. Row-reduce the matrix to $[I_n\,|\,A^{-1}]$.
3. The matrix $A^{-1}$ is the $n\times n$ matrix on the right side of the augmented matrix.

Ex.
$$
\begin{align*}
\text{Find the inverse of }&\begin{bmatrix}1 & 2\\3 & 4\end{bmatrix}.\\
\left[\begin{array}{cc|cc}1 & 2 & 1 & 0\\3 & 4 & 0 & 1\end{array}\right]&\rightarrow
\left[\begin{array}{cc|cc}1 & 2 & 1 & 0\\0 & -2 & -3 & 1\end{array}\right]\rightarrow\\
\left[\begin{array}{cc|cc}1 & 2 & 1 & 0\\0 & 1 & 3/2 & -1/2\end{array}\right]&\rightarrow
\left[\begin{array}{cc|cc}1 & 0 & -2 & 1\\0 & 1 & 3/2 & -1/2\end{array}\right]\rightarrow\\
\text{So, the inverse is }&\boxed{\begin{bmatrix}-2 & 1\\3/2 & -1/2\end{bmatrix}}.
\end{align*}
$$
It's good practice to verify results, so you should multiply the inverse by the original matrix to check that the result is the identity matrix.

As a side note, for $2\times 2$ matrices, we can find the determinant directly with the formula:
$$
\begin{bmatrix}a & b \\ c & d\end{bmatrix}^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d & -b\\ -c & a\end{bmatrix}.
$$

**Invertible Matrix Theorem**<br />
For any $n\times n$ matrix $A$, the following are equivalent:
1. $A$ is invertible.
2. $A$ is row equivalent to $I_n$.
3. $A$ has $n$ pivot positions.
4. The equation $Ax=0$ has only the trivial solution.
5. The columns of $A$ are linearly independent.
6. The equation $Ax=b$ has at least one solution for each $b$ in $\mathbb{R}^n$.
7. The columns of $A$ span $\mathbb{R}^n$.
8. The equation $Ax=b$ has exactly one solution for each $b$ in $\mathbb{R}^n$.
9. The determinant of $A$ is nonzero.
10. The transpose of $A$ is invertible.

You don't need to memorize each one of these, but do take the time to understand *why* they are true — this will help you remember them.

<hr >
# Change of Basis

If $\mathbb{V}$ is a vector space with basis $B$ and $\mathbb{W}$ is a basis with basis $C$, and $T:\mathbb{V}\to \mathbb{W}$ is linear, then there exists a matrix $A_B^C$ such that $(Tv)_C=A_B^C(v)_B$. The $j$th column consists of the coordinate representation of the $j$th vector in $\mathbb{V}$'s basis expressed with respect to basis $C$.
This is just a more general form of how we've been talking about transformations: before, we weren't including bases, but now we're explicitly stating that the transformation is from one basis to another.
If $B$ and $C$ are two bases of vector space $\mathbb{V}$, then there exists a **transition matrix** $P_B^C$ (read: "$P$ from $B$ to $C$") such that $(v)_C=P_B^C (v)_B$ for all $v\in \mathbb{V}$.

If $\mathbb{V}$ is a vector space with bases $B$ and $C$ and $T:\mathbb{V}\to \mathbb{V}$ is linear, then $P_B^CA_B^B=A_C^CP_B^C$.

**So how do we change basis?**<br />
We can change basis by multiplying by the transition matrix! If we have a vector $v$ in basis $B$ (denoted "$(v)_B$"), we can find its representation in basis $C$ by multiplying by $P_B^C$. So, $(v)_C=P_B^C(v)_B$.
That's the same thing as a 2d transformation! We use the original coordinate system, and transform the basis vectors to the other system's coordinates. So, changing basis vectors is *matrix-vector multiplication*, where the matrices' columns are how we would express the "other" basis vectors!

**Change of Basis for Transformations**<br />
We can change the basis that a transformation is represented in the same way we would change basis for vectors:
$$
A_C^C=P_B^CA_B^BP_B^C.
$$
If you read this out loud, it'll make sense: "$A$ from base $C$ to $C$ is equal to the transition matrix from $C$ to $B$ times $A$ from $B$ to $B$ times the transition matrix from $B$ to $C$."
We'll see an incredibly powerful application of this in the [Diagonalization section](/notes/linalg/diagonalization).



<hr >
# Eigenvectors, Values, Bases, and Spaces

**Eigenvectors**<br />
Eigenvectors of a transformation are vectors that stay on their span during the transformation. There are two similar ways of defining them: through transformations and matrices. The gist is the same: a nonzero (important!) vector $v\in\mathbb{R}^n$ (matrix) or $v\in\mathbb{V}$ (transformation) is an eigenvector of $A$ (matrix) or $T:\mathbb{V}\to\mathbb{V}$ (linear transformation) if
1. $Av=\lambda v$, or
2. $Tv=\lambda v$.
$\lambda$ is the eigenvector's...

**Eigenvalue**<br />
An eigenvalue describes the scalar that the eigenvector is multiplied by as a result of the transformation. Why are eigenvectors important? Consider a 3d rotation — if you can find an eigenvector, you have found the axis of rotation for that transformation.

**Eigenbasis**<br />
Whenever a matrix has zeroes everywhere except the diagonal, it's called a **diagonal matrix**. It means that every single basis vector is an eigenvector! Diagonal matrices allow you to do a lot — computations with them are very easy. But... isn't it unlikely that you'll get a diagonal matrix as your transformation? Well, funny thing — if you can find a set of eigenvectors that span space, you can change basis *to those eigenvectors* to get a diagonal transformation matrix!

**Finding eigenthings**<br />
$\lambda$ is an eigenvalue of the $n\times n$ matrix $A$ if and only if
$$\det(A-\lambda I)=0.$$
The above equation is called the **characteristic** equation of the matrix $A$. Solving the characteristic equation yields some number of eigenvalues $\lambda_1,\dots,\lambda_k$. We can find each eigenvalue's eigenspace by solving the equation $Ax=\lambda x$. This is equivalent to solving
$$(A-\lambda I_n)x=0.$$

**Theorems**<br />
There are a ton of results related to eigenvalues and eigenvectors. Here are a few. Try to prove each one, or at least understand why they're true.

Thm. If $\mathbb{V}$ is finite dimensional and $:\mathbb{V}\to\mathbb{V}$ is linear, then there exists a basis $B$ of $\mathbb{V}$ such that the representation of $T$ with respect to $B$ is a diagonal matrix if and only if there exists a basis of $\mathbb{V}$ consisting of only eigenvectors of $T$.

Thm. $u\in \mathbb{V}$ is an eigenvector with eigenvalue $\lambda$ for a given transformation $T:\mathbb{V}\to\mathbb{V}$ if and only if for any basis $C$ of $\mathbb{V}$ and matrix representation $A_C^C$ of $T$, $(u)_C$ is an eigenvector with eigenvalue $\lambda$ for $A_C^C$. So, to find all eigenvectors and eigenvalues of a transformation, it suffices to find all eigenvectors and eigenvalues of any matrix representation of that transformation.

Thm. For a linear transformation $T:\mathbb{V}\to\mathbb{V}$ with eigenvalue $\lambda$, the set
$$
S_\lambda=\left\{v\in\mathbb{V}\;|\; Tv=\lambda v\right\}=\left\{\textbf{0}\right\}\cup\left\{v\in\mathbb{V}\;|v\text{ is an eigenvector with eigenvalue }\lambda\right\}$$
is a subspace of $\mathbb{V}$. This set is called the **eigenspace** of $T$ with eigenvalue $\lambda$.

Thm. If $A$ and $B$ are similar matrices, then they have the same characteristic equation, and, as such, the same eigenvalues.

Thm. For an upper or lower triangular matrix, the eigenvalues are the diagonal elements.

Thm. $A$ is singular if and only if it has a zero eigenvalue (obviously: $\det(A-0 I_n)=0$).

Thm. If $x$ is an eigenvector of an invertible $n\times n$ matrix $A$ with eigenvalue $\lambda$, then $x$ is an eigenvector of $A^{-1}$ with eigenvalue $1/\lambda$.

Thm. If $x$ is an eigenvector of $A$ with eigenvalue $\lambda$, then
1. $x$ is an eigenvector of $kA$ with eigenvalue $k\lambda$ for some scalar $k$.
2. $x$ is an eigenvector of $A^n$ with eigenvalue $\lambda^n$ for each positive integer $n$.

**Trace**<br />
The trace of an $n\times n$ matrix $A=[a_{ij}]$ is
$$tr(A)=\sum_{i=1}^n a_{ii},$$
which is just the sum of the diagonal elements.

Thm. Suppose $A$ is an $n\times n$ matrix with eigenvalues $\lambda_1, \dots, \lambda_n$ (possibly complex), listed with multiplicity. Then
1. $tr(A)=\lambda_1 + \lambda_2 + \dots + \lambda_n$.
2. $\det(A)=\lambda_1\lambda_2\dots\lambda_n$.


<hr />

**Quick Trick for Finding $2\times2$ Eigenvalues**<br />
There are a few things to know.
1. $\text{tr}\left(\begin{bmatrix}a & b\\c & d\end{bmatrix}\right)=a+d=\lambda_1+\lambda_2$, so the average of these two diagonal entries is the same as the average of the two eigenvalues.
2. $\det\left(\begin{bmatrix}a & b \\ c & d\end{bmatrix}\right)=ad-bc=\lambda_1\lambda_2$.
3. $\lambda_1, \lambda_2 = m\pm \sqrt{m^2-p}$, where $m$ is the mean of the diagonals and $p$ is the determinant of the matrix.

This is from a [3Blue1Brown video](https://www.youtube.com/watch?v=e50Bj7jn9IQ), and it's a really cool trick! It's a good way to check your work, and it's a good way to find eigenvalues quickly.

<hr >



# Diagonalization (Applications of Eigenthings)

Diagonalization is used all the time in computer graphics, physics, and engineering. It's a way of simplifying a transformation by changing the basis to one of eigenvectors. This is a powerful tool, and it's a great way to understand the power of eigenvectors and eigenvalues.

**Diagonalizability**<br />
A square matrix is diagonalisable if it is similar to a diagonal matrix.

<hr />

Thm. An $n\times n$ matrix $A$ is diagonalisable if and only if it has $n$ linearly independent eigenvectors.

While we can't guarantee that $A$ is similar to a unique diagonal matrix, we can guarantee the following:
If $A$ is similar to the diagonal matrices $D_1$ and $D_2$, then $D_1$ and $D_2$ have the same set of diagonal elements (with the same multiplicities).

Thm. Let $A$ be a square matrix. For each positive integer $k$, if $x_1,\dots,x_k$ is are eigenvectors of $A$ with distinct eigenvalues $\lambda_1,\dots,\lambda_k$, then $\left\{x_1,\dots,x_k\right\}$ is linearly independent (this is just saying that eigenvectors with distinct eigenvalues are linearly independent).

Thm. If $A$ is $n\times n$ and $A$ has $n$ distinct real eigenvalues, then $A$ is diagonalizable.

But what if $A$ doesn't have $n$ distinct real eigenvalues? It may still be diagonlizable:

Thm. If $A$ is an $n\times n$ matrix with with real eigenvalues $\lambda_1,\dots,\lambda_k$, and $S_{\lambda_j}$ denotes the eigenspace of $\lambda_j$ for each $\lambda_j$. Then, $A$ is diagonalizable if and only if $$\sum_{i=1}^k \dim(S_{\lambda_i})=n.$$
Thm. If $\lambda$ is an eigenvalue of the $n\times n$ matrix $A$, then
$$\dim(S_\lambda)=n-r(A-\lambda I_n)$$
because $A-\lambda I_n$ is the kernel of $A:\mathbb{R}^n\to\mathbb{R}^n$.

<hr />

**What's the point of diagonalization?**<br />
Let's consider some matrix $A$ that represents a transformation. What if we want to apply it three times ($A^3$)? Well, we could multiply $A$ by itself three times, but that's a lot of work.
If we can find a diagonal matrix $D$ that is similar to $A$, then $A=PDP^{-1}$, and $A^3=PD^3P^{-1}$. But $D^3$ is just the diagonal elements cubed! So, we can find $A^3$ by just cubing the diagonal elements of $D$.
Diagonalization also makes it easy to find the inverse of a matrix. If $A=PDP^{-1}$, then $A^{-1}=PD^{-1}P^{-1}$, and $D^{-1}$ is just the reciprocal of the diagonal elements.

<hr />

*~ The End ~*

What I've covered here is just the beginning of linear algebra. There's so much more to learn, and I hope you continue to explore. If you have any questions, feel free to reach out to me at [my email](mailto:boris.nezlobin@gmail.com). Good luck!